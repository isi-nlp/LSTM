SAMPLE SCRIPTS FOR TRAINING A SINGLE LAYER LSTM SEQUENCE-TO-SEQUENCE-MODEL

--train a japanese phoneme to english phoneme sequence model. The validation perplexity should be around 1.6
./train_e2j.sh 


--train a japanese phoneme to english phoneme sequence model with dropout. The validation perplexity should be around 1.5
./train_e2j_dropout.sh

--compute the probability of pairs of phoneme sequences with a trained model. Here, we score with the dropout trained models generated by train_e2j_dropout.sh. The perplexity on the test sequence (e2j.test.score.carmel) should be around 1.15
./score_e2j.sh lstm.s2s.e2j.dropout.decoder.best lstm.s2s.e2j.dropout.encoder.best

--given input sequences, generate the output sequence greedily one phoneme at a time
./generate_e2j_greedy.sh lstm.s2s.e2j.dropout.decoder.best lstm.s2s.e2j.dropout.encoder.best

--given input sequences, generate the output sequence with beam search
./generate_e2j_beam.sh lstm.s2s.e2j.dropout.decoder.best lstm.s2s.e2j.dropout.encoder.best



SAMPLE SCRIPTS FOR TRAINING A SINGLE LAYER LSTM LANGUAGE MODEL

--train and test a language model trained with Noise Contrastive Estimation (Gutmann et al., 2012). The script trains a model with 10 noise samples 
./trainInfernoNCE.sh
./testInfernoNCE.sh


--train and test a language model trained with Maximum Likelihood Estimation. :w
./trainInfernoMLE.sh
./testInfernoMLE.sh

